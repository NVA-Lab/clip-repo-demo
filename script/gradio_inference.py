import autorootcwd
import gradio as gr
import torch
import clip
from PIL import Image
import numpy as np
import os
from script.pretrain import ClipFinetuner

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# ì „ì—­ ë³€ìˆ˜
model = None
preprocess = None
is_finetuned = False

def load_model():
    """ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global model, preprocess, is_finetuned
    
    if model is None:
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ê¸°ë³¸ CLIP ëª¨ë¸ ë¡œë“œ
        base_clip_model, preprocess = clip.load("ViT-B/32", device=device)
        
        # Fine-tuned ëª¨ë¸ ë¡œë“œ ì‹œë„
        checkpoint_path = "logs/clip_pretrain/version_0/checkpoints/epoch=0-step=1000.ckpt"
    
        clip_finetuner = ClipFinetuner.load_from_checkpoint(
            checkpoint_path,
            clip_model=base_clip_model
        )
        model = clip_finetuner.clip_model
        is_finetuned = True
        print("âœ… Fine-tuned ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        model.eval()
        model = model.to(device)
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Fine-tuned: {is_finetuned}, Device: {device})")

def quick_detect(image):
    """ë¹ ë¥¸ ê°•ì•„ì§€/ê³ ì–‘ì´ ê°ì§€"""
    if image is None:
        return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    load_model()
    
    # ì „ì²˜ë¦¬
    image_tensor = preprocess(image).unsqueeze(0).to(device)
    labels = ["a dog", "a cat", "neither dog nor cat"]
    text = clip.tokenize(labels).to(device)
    
    with torch.no_grad():
        image_features = model.encode_image(image_tensor)
        text_features = model.encode_text(text)
        
        # ì •ê·œí™”
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        logits_per_image = 100 * image_features @ text_features.t()
        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]
    
    # ê²°ê³¼ í¬ë§·íŒ…
    results = []
    for label, prob in zip(labels, probs):
        results.append(f"**{label}**: {prob:.2%}")
    
    # ìµœê³  ì˜ˆì¸¡
    best_idx = np.argmax(probs)
    best_result = f"\nğŸ¯ **ì˜ˆì¸¡ ê²°ê³¼: {labels[best_idx]}** (ì‹ ë¢°ë„: {probs[best_idx]:.2%})"
    
    model_info = f"ğŸ“± ëª¨ë¸: {'Fine-tuned CLIP' if is_finetuned else 'Base CLIP'} | ë””ë°”ì´ìŠ¤: {device}\n\n"
    
    return model_info + "\n".join(results) + best_result

def custom_predict(image, desc1, desc2, desc3):
    """ì»¤ìŠ¤í…€ 3ê°œ ì„¤ëª… ì˜ˆì¸¡"""
    if image is None:
        return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    descriptions = [desc1.strip(), desc2.strip(), desc3.strip()]
    
    if not all(descriptions):
        return "ëª¨ë“  ì„¤ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    load_model()
    
    # ì „ì²˜ë¦¬
    image_tensor = preprocess(image).unsqueeze(0).to(device)
    text = clip.tokenize(descriptions).to(device)
    
    with torch.no_grad():
        image_features = model.encode_image(image_tensor)
        text_features = model.encode_text(text)
        
        # ì •ê·œí™”
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        logits_per_image = 100 * image_features @ text_features.t()
        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]
    
    # ê²°ê³¼ë¥¼ í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
    results = list(zip(descriptions, probs))
    results.sort(key=lambda x: x[1], reverse=True)
    
    # ê²°ê³¼ í¬ë§·íŒ…
    output = f"ğŸ“± ëª¨ë¸: {'Fine-tuned CLIP' if is_finetuned else 'Base CLIP'} | ë””ë°”ì´ìŠ¤: {device}\n\n"
    output += "ğŸ“Š **ë¶„ì„ ê²°ê³¼** (í™•ë¥  ìˆœ):\n\n"
    
    for i, (desc, prob) in enumerate(results):
        if i == 0:
            output += f"ğŸ¥‡ **{desc}**: {prob:.2%}\n"
        elif i == 1:
            output += f"ğŸ¥ˆ **{desc}**: {prob:.2%}\n"
        else:
            output += f"ğŸ¥‰ **{desc}**: {prob:.2%}\n"
    
    # ìµœê³  ì˜ˆì¸¡ ê°•ì¡°
    best_desc, best_prob = results[0]
    output += f"\nğŸ¯ **ëª¨ë¸ì˜ ìµœì¢… ì„ íƒ**: '{best_desc}' (ì‹ ë¢°ë„: {best_prob:.2%})"
    
    return output

# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_interface():
    with gr.Blocks(title="ğŸ•ğŸ± CLIP Dog & Cat Detection", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ•ğŸ± CLIPì„ ì´ìš©í•œ ê°•ì•„ì§€/ê³ ì–‘ì´ ê°ì§€
            
            Fine-tuned CLIP ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
            """
        )
        
        with gr.Tab("ğŸš€ ë¹ ë¥¸ ê°ì§€"):
            gr.Markdown("### ê°•ì•„ì§€ì¸ì§€ ê³ ì–‘ì´ì¸ì§€ ë¹ ë¥´ê²Œ ê°ì§€í•´ë³´ì„¸ìš”!")
            
            with gr.Row():
                with gr.Column():
                    image_input1 = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
                    detect_btn = gr.Button("ğŸ” ê°ì§€í•˜ê¸°", variant="primary")
                
                with gr.Column():
                    quick_output = gr.Markdown(label="ê²°ê³¼")
            
            detect_btn.click(
                fn=quick_detect,
                inputs=[image_input1],
                outputs=[quick_output]
            )
        
        with gr.Tab("ğŸ® ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"):
            gr.Markdown(
                """
                ### 2ê°œì˜ ê±°ì§“ê³¼ 1ê°œì˜ ì§„ì‹¤
                ì´ë¯¸ì§€ì— ëŒ€í•œ 3ê°œì˜ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. ëª¨ë¸ì´ ì–´ë–¤ ì„¤ëª…ì„ ê°€ì¥ ì í•©í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”!
                """
            )
            
            with gr.Row():
                with gr.Column():
                    image_input2 = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
                    
                    desc1_input = gr.Textbox(
                        label="ì„¤ëª… 1", 
                        placeholder="ì˜ˆ: ë¹¨ê°„ ì‚¬ê³¼",
                        lines=2
                    )
                    desc2_input = gr.Textbox(
                        label="ì„¤ëª… 2", 
                        placeholder="ì˜ˆ: ì°¨ê³ ì— ì£¼ì°¨ëœ ìë™ì°¨",
                        lines=2
                    )
                    desc3_input = gr.Textbox(
                        label="ì„¤ëª… 3", 
                        placeholder="ì˜ˆ: ë‚˜ë¬´ì— ë‹¬ë¦° ì˜¤ë Œì§€",
                        lines=2
                    )
                    
                    predict_btn = gr.Button("ğŸ”® ì˜ˆì¸¡í•˜ê¸°", variant="primary")
                
                with gr.Column():
                    custom_output = gr.Markdown(label="ë¶„ì„ ê²°ê³¼")
            
            predict_btn.click(
                fn=custom_predict,
                inputs=[image_input2, desc1_input, desc2_input, desc3_input],
                outputs=[custom_output]
            )
        
        with gr.Tab("â„¹ï¸ ì •ë³´"):
            gr.Markdown(
                """
                ### ì‚¬ìš©ë²•
                
                **ë¹ ë¥¸ ê°ì§€ íƒ­:**
                - ê°•ì•„ì§€ë‚˜ ê³ ì–‘ì´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  'ê°ì§€í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­
                - ëª¨ë¸ì´ ê°•ì•„ì§€/ê³ ì–‘ì´/ê¸°íƒ€ ì¤‘ì—ì„œ ì„ íƒ
                
                **ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ íƒ­:**
                - ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  3ê°œì˜ ì„¤ëª…ì„ ì…ë ¥
                - ëª¨ë¸ì´ ì–´ë–¤ ì„¤ëª…ì´ ê°€ì¥ ì í•©í•œì§€ íŒë‹¨
                - "2ê°œì˜ ê±°ì§“ê³¼ 1ê°œì˜ ì§„ì‹¤" ê²Œì„ì²˜ëŸ¼ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤
                
                ### íŒ
                - ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”
                - ëª¨ë¸ì€ ì‹œê°ì  íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤
                - Fine-tuned ëª¨ë¸ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤
                """
            )
    
    return demo

if __name__ == "__main__":
    # ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    demo = create_interface()
    demo.launch(
        share=True,  # ê³µê°œ ë§í¬ ìƒì„±
        server_name="0.0.0.0",  # ì™¸ë¶€ ì ‘ì† í—ˆìš©
        server_port=7860,
        show_error=True
    )